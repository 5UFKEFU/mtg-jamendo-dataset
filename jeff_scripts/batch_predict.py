#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡éŸ³ä¹è‡ªåŠ¨æ ‡ç­¾è„šæœ¬ï¼ˆå¤šæ¨¡å‹å¯é€‰ç‰ˆï¼‰

ç¤ºä¾‹ï¼š
    python batch_predict.py --dataset jamendo --model musicnn --audio_folder my_music
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ä¾èµ– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import os, sys, json, argparse, subprocess
from datetime import datetime

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchaudio
from tabulate import tabulate

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ åŠ¨æ€å¯¼å…¥å®˜æ–¹æ¨¡å‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
REPO_PATH = os.path.abspath("../sota-music-tagging-models")     # â† æŒ‰å®é™…è·¯å¾„è°ƒæ•´
TRAIN_PATH = os.path.join(REPO_PATH, "training")
if TRAIN_PATH not in sys.path:
    sys.path.insert(0, TRAIN_PATH)

try:
    import model as md        # å®˜æ–¹æ‰€æœ‰æ¨¡å‹ç±»éƒ½åœ¨ training/model.py
except Exception as e:
    raise RuntimeError(
        f"æ— æ³•å¯¼å…¥å®˜æ–¹æ¨¡å‹ï¼š{e}\n"
        "è¯·ç¡®è®¤ REPO_PATH æŒ‡å‘æ­£ç¡®ï¼Œå¹¶å·²æ‰§è¡Œ\n"
        "  pip install -e ../sota-music-tagging-models"
    )

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CLI å‚æ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def parse_args():
    ap = argparse.ArgumentParser("batch music tagging")
    ap.add_argument("--dataset", required=True, help="jamendo | mtat | msd")
    ap.add_argument("--model",   required=True,
                    help="attention | crnn | fcn | hcnn | musicnn | musicnn600 | sample | se | short | short_res")
    ap.add_argument("--audio_folder", default="my_music")
    ap.add_argument("--top_k", type=int, default=5)
    ap.add_argument("--threshold", type=float, default=0.1)
    ap.add_argument("--exts", nargs="+", default=[".wav"], help=".wav .mp3 â€¦")
    ap.add_argument("--model_root", default="../sota-music-tagging-models/models")
    return ap.parse_args()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ è®¾å¤‡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
torch.backends.cudnn.benchmark = True

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ æ ‡ç­¾ï¼ˆJamendoâ€‘56ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
id_to_label = [
    ("classical", "å¤å…¸"), ("electronic", "ç”µå­"), ("experimental", "å®éªŒ"), ("folk", "æ°‘è°£"),
    ("hip-hop", "å˜»å“ˆ"), ("instrumental", "å™¨ä¹"), ("international", "ä¸–ç•ŒéŸ³ä¹"), ("pop", "æµè¡Œ"),
    ("rock", "æ‘‡æ»š"), ("jazz", "çˆµå£«"), ("happy", "å¿«ä¹"), ("sad", "æ‚²ä¼¤"), ("angry", "æ„¤æ€’"),
    ("relaxed", "æ”¾æ¾"), ("romantic", "æµªæ¼«"), ("epic", "å²è¯—"), ("dark", "é»‘æš—"), ("fun", "æœ‰è¶£"),
    ("calm", "å¹³é™"), ("acoustic guitar", "æœ¨å‰ä»–"), ("electric guitar", "ç”µå‰ä»–"), ("piano", "é’¢ç´"),
    ("synthesizer", "åˆæˆå™¨"), ("violin", "å°æç´"), ("drums", "é¼“"), ("bass", "è´æ–¯"),
    ("vocals", "äººå£°"), ("male vocal", "ç”·å£°"), ("female vocal", "å¥³å£°"), ("choral", "åˆå”±"),
    ("spoken word", "å£è¯­"), ("a cappella", "æ— ä¼´å¥åˆå”±"), ("beatboxing", "å£æŠ€"),
    ("autotune", "ç”µéŸ³ä¿®éŸ³"), ("shouting", "å–Šå«"), ("whistling", "å£å“¨"), ("party", "æ´¾å¯¹"),
    ("travel", "æ—…è¡Œ"), ("sleep", "ç¡çœ "), ("study", "å­¦ä¹ "), ("workout", "å¥èº«"),
    ("meditation", "å†¥æƒ³"), ("commercial", "å•†ä¸š"), ("children", "å„¿ç«¥"), ("holiday", "å‡æ—¥"),
    ("wedding", "å©šç¤¼"), ("vintage", "å¤å¤"), ("modern", "ç°ä»£"), ("retro", "æ€€æ—§"),
    ("ambient", "æ°›å›´"), ("cinematic", "ç”µå½±æ„Ÿ"), ("lo-fi", "ä½ä¿çœŸ")
]

tag_groups = {
    "genre": ["classical", "electronic", "experimental", "folk", "hip-hop",
              "instrumental", "international", "pop", "rock", "jazz"],
    "mood": ["happy", "sad", "angry", "relaxed", "romantic", "epic", "dark", "fun", "calm"],
    "instrument": ["acoustic guitar", "electric guitar", "piano", "synthesizer",
                   "violin", "drums", "bass", "vocals", "male vocal", "female vocal"],
    "vocal style": ["choral", "spoken word", "a cappella", "beatboxing",
                    "autotune", "shouting", "whistling"],
    "scene": ["party", "travel", "sleep", "study", "workout", "meditation",
              "commercial", "children", "holiday", "wedding"],
    "style": ["vintage", "modern", "retro", "ambient", "cinematic", "lo-fi"]
}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ éŸ³é¢‘é¢„å¤„ç† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def preprocess_audio(path, sr_out=16000, dur=30):
    wav, sr = torchaudio.load(path)
    if sr != sr_out:
        wav = torchaudio.transforms.Resample(sr, sr_out)(wav)
    need = sr_out * dur
    if wav.shape[1] < need:
        wav = F.pad(wav, (0, need - wav.shape[1]))
    else:
        wav = wav[:, :need]
    return wav

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ å®ä¾‹åŒ–æ¨¡å‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MODEL_NAMES = {
    "crnn":       "CRNN",
    "fcn":        "FCN",
    "sample":     "SampleCNN",
    "se":         "SampleCNN_SE",      # æºç ä½¿ç”¨ä¸‹åˆ’çº¿
    "short":      "ShortChunkCNN",
    "short_res":  "ShortChunkCNN_Res",
    "attention":  "CNNSA",
    "hcnn":       "HarmonicCNN",
    "musicnn":    "Musicnn",
    "musicnn600": "Musicnn",
}

def build_model(model_key, n_classes, dataset):
    if model_key not in MODEL_NAMES:
        raise ValueError(f"æœªçŸ¥æ¨¡å‹: {model_key}")

    cls_name = MODEL_NAMES[model_key]
    mdl_cls = getattr(md, cls_name)

    # å„æ¨¡å‹æ„é€ å‡½æ•°ç•¥æœ‰ä¸åŒï¼Œåšä¸€ä¸ª tryâ€‘chain
    for kwargs in (
        dict(n_classes=n_classes, dataset=dataset),
        dict(num_classes=n_classes, dataset=dataset),
        dict(n_classes=n_classes),
        dict(num_classes=n_classes),
        dict(dataset=dataset),
        {}
    ):
        try:
            return mdl_cls(**kwargs)
        except TypeError:
            continue
    raise RuntimeError(f"æ— æ³•å®ä¾‹åŒ– {cls_name}")

def load_model(weight_path, model_key, dataset):
    if not os.path.isfile(weight_path):
        raise FileNotFoundError(weight_path)
    state = torch.load(weight_path, map_location=device)

    # ç”¨æœ€åä¸€ä¸ªå…¨è¿æ¥çš„ bias æ¨æ–­ç±»åˆ«æ•°
    n_classes = max(v.shape[0] for k, v in state.items() if k.endswith(".bias") and v.ndim == 1)
    net = build_model(model_key, n_classes, dataset)

    # æŸäº›æ¨¡å‹ä¿å­˜äº†é¢å¤–çš„ buffersï¼ˆå¦‚ MelÂ æ»¤æ³¢bankï¼‰
    missing, unexpected = net.load_state_dict(state, strict=False)
    if missing:
        print("âš ï¸  State missing:", missing)
    if unexpected:
        print("âš ï¸  State unexpected:", unexpected)

    if torch.cuda.device_count() > 1:
        print(f"ğŸš€  ä½¿ç”¨ {torch.cuda.device_count()} å¼  GPU å¹¶è¡Œ")
        net = nn.DataParallel(net)
    return net.to(device).eval()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ é¢„æµ‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def predict_one(path, net, top_k, th):
    wav = preprocess_audio(path).to(device)
    with torch.no_grad():
        prob = torch.sigmoid(net(wav)).squeeze().cpu()

    preds = []
    for i, p in enumerate(prob.tolist()):
        if p < th:
            continue
        if i < len(id_to_label):
            en, zh = id_to_label[i]
        else:               # 600 ç±»æ—¶æ— ä¸­æ–‡
            en, zh = f"tag_{i}", f"tag_{i}"
        preds.append({"tag": en, "zh": zh, "prob": round(p, 3)})
    preds.sort(key=lambda x: x["prob"], reverse=True)
    return preds[:top_k]

def group_preds(preds):
    grp = {g: [] for g in tag_groups}
    grp["other"] = []
    for p in preds:
        for g, lst in tag_groups.items():
            if p["tag"].lower() in [t.lower() for t in lst]:
                grp[g].append(p); break
        else:
            grp["other"].append(p)
    return grp

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ è¾…åŠ© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def find_audio(root, exts):
    exts = tuple(e.lower() for e in exts)
    return [os.path.join(r, f)
            for r, _, fs in os.walk(root)
            for f in fs if f.lower().endswith(exts)]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ä¸»ç¨‹åº â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    args = parse_args()
    ckpt = os.path.join(args.model_root, args.dataset, args.model, "best_model.pth")
    net  = load_model(ckpt, args.model, args.dataset)

    files = find_audio(args.audio_folder, args.exts)
    if not files:
        print("âš ï¸  æœªæ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶"); return
    print(f"ğŸ§  å…± {len(files)} é¦–éŸ³é¢‘ï¼Œå¼€å§‹åˆ†æ â€¦")

    results = {}
    for f in files:
        try:
            ps = predict_one(f, net, args.top_k, args.threshold)
            gp = group_preds(ps); results[f] = gp
            rows = [[g, p["tag"], p["zh"], p["prob"]] for g, ts in gp.items() for p in ts]
            print(f"\nğŸµ {os.path.basename(f)}")
            print(tabulate(rows, headers=["ç±»åˆ«", "EN", "ä¸­æ–‡", "æ¦‚ç‡"], tablefmt="grid") if rows else "  <æ— æ ‡ç­¾>")
        except Exception as e:
            print(f"âŒ  {f}: {e}")

    out = f"results_{args.dataset}_{args.model}_{datetime.now():%Y%m%d_%H%M%S}.json"
    with open(out, "w", encoding="utf-8") as fp:
        json.dump(results, fp, indent=2, ensure_ascii=False)
    print(f"\nâœ…  å®Œæˆï¼Œç»“æœä¿å­˜åœ¨ {out}")

if __name__ == "__main__":
    main()