import torch
import torchaudio
import torch.nn as nn
import torch.nn.functional as F
import subprocess
import os
import sys
id_to_label = [
    ("classical", "å¤å…¸"), ("electronic", "ç”µå­"), ("experimental", "å®éªŒ"), ("folk", "æ°‘è°£"), ("hip-hop", "å˜»å“ˆ"),
    ("instrumental", "å™¨ä¹"), ("international", "ä¸–ç•ŒéŸ³ä¹"), ("pop", "æµè¡Œ"), ("rock", "æ‘‡æ»š"), ("jazz", "çˆµå£«"),

    ("happy", "å¿«ä¹"), ("sad", "æ‚²ä¼¤"), ("angry", "æ„¤æ€’"), ("relaxed", "æ”¾æ¾"), ("romantic", "æµªæ¼«"),
    ("epic", "å²è¯—"), ("dark", "é»‘æš—"), ("fun", "æœ‰è¶£"), ("calm", "å¹³é™"),

    ("acoustic guitar", "æœ¨å‰ä»–"), ("electric guitar", "ç”µå‰ä»–"), ("piano", "é’¢ç´"), ("synthesizer", "åˆæˆå™¨"),
    ("violin", "å°æç´"), ("drums", "é¼“"), ("bass", "è´æ–¯"), ("vocals", "äººå£°"),
    ("male vocal", "ç”·å£°"), ("female vocal", "å¥³å£°"),

    ("choral", "åˆå”±"), ("spoken word", "å£è¯­"), ("a cappella", "æ— ä¼´å¥åˆå”±"), ("beatboxing", "å£æŠ€"),
    ("autotune", "ç”µéŸ³ä¿®éŸ³"), ("shouting", "å–Šå«"), ("whistling", "å£å“¨"),

    ("party", "æ´¾å¯¹"), ("travel", "æ—…è¡Œ"), ("sleep", "ç¡çœ "), ("study", "å­¦ä¹ "), ("workout", "å¥èº«"),
    ("meditation", "å†¥æƒ³"), ("commercial", "å•†ä¸š"), ("children", "å„¿ç«¥"), ("holiday", "å‡æ—¥"), ("wedding", "å©šç¤¼"),

    ("vintage", "å¤å¤"), ("modern", "ç°ä»£"), ("retro", "æ€€æ—§"), ("ambient", "æ°›å›´"), ("cinematic", "ç”µå½±æ„Ÿ"), ("lo-fi", "ä½ä¿çœŸ")
]

def inspect_model_weights(pth_file):
    state_dict = torch.load(pth_file, map_location='cpu')
    print(f"âœ… æ¨¡å‹åŒ…å« {len(state_dict)} ä¸ªå‚æ•°å±‚ï¼š\n")
    for name, param in state_dict.items():
        print(f"{name:35} {tuple(param.shape)}")

def auto_convert_wav(input_path, output_path="fixed.wav"):
    if not os.path.exists(input_path):
        raise FileNotFoundError(f"File {input_path} not found.")
    command = [
        "ffmpeg", "-y", "-i", input_path,
        "-ar", "16000", "-ac", "1", "-c:a", "pcm_s16le",
        output_path
    ]
    result = subprocess.run(command, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    if result.returncode != 0:
        raise RuntimeError("ffmpeg è½¬æ¢éŸ³é¢‘å¤±è´¥ï¼Œè¯·ç¡®ä¿å·²æ­£ç¡®å®‰è£…ã€‚")
    return output_path

def preprocess_audio(audio_path, target_sr=16000, duration=10):
    try:
        waveform, sr = torchaudio.load(audio_path)
    except RuntimeError as e:
        print("âŒ torchaudio æ— æ³•åŠ è½½éŸ³é¢‘ã€‚è¯·å°è¯•å®‰è£…ä¾èµ–ï¼š")
        print("   ğŸ‘‰ yum install libsndfile")
        print("   ğŸ‘‰ pip install soundfile")
        raise e

    if sr != target_sr:
        waveform = torchaudio.transforms.Resample(sr, target_sr)(waveform)

    expected_len = target_sr * duration
    if waveform.shape[1] < expected_len:
        pad_len = expected_len - waveform.shape[1]
        waveform = F.pad(waveform, (0, pad_len))
    else:
        waveform = waveform[:, :expected_len]
    return waveform

class CNNMusicTagger(nn.Module):
    def __init__(self, n_classes=56):
        super().__init__()
        self.bn_init = nn.BatchNorm2d(1)

        self.conv_1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)
        self.bn_1 = nn.BatchNorm2d(64)

        self.conv_2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.bn_2 = nn.BatchNorm2d(128)

        self.conv_3 = nn.Conv2d(128, 128, kernel_size=3, padding=1)
        self.bn_3 = nn.BatchNorm2d(128)

        self.conv_4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)
        self.bn_4 = nn.BatchNorm2d(128)

        self.conv_5 = nn.Conv2d(128, 64, kernel_size=3, padding=1)
        self.bn_5 = nn.BatchNorm2d(64)

        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))
        self.dense = nn.Linear(64, n_classes)

    def forward(self, x):
        x = self._wav_to_spectrogram(x)
        x = self.bn_init(x)

        x = F.relu(self.bn_1(self.conv_1(x)))
        x = F.relu(self.bn_2(self.conv_2(x)))
        x = F.relu(self.bn_3(self.conv_3(x)))
        x = F.relu(self.bn_4(self.conv_4(x)))
        x = F.relu(self.bn_5(self.conv_5(x)))

        x = self.global_pool(x).squeeze(-1).squeeze(-1)
        x = self.dense(x)
        return x

    def _wav_to_spectrogram(self, x):
        if x.dim() == 2:
            x = x.unsqueeze(1)  # [B, 1, T]
        spec = torchaudio.transforms.MelSpectrogram(
            sample_rate=16000,
            n_fft=1024,
            hop_length=512,
            n_mels=128
        )(x.squeeze(1))  # [B, mel, time]
        spec = torchaudio.transforms.AmplitudeToDB()(spec)
        return spec.unsqueeze(1)  # [B, 1, mel, time]

def load_model(model_path):
    model = CNNMusicTagger(n_classes=56)
    model.load_state_dict(torch.load(model_path, map_location='cpu'))
    model.eval()
    return model

def predict_tags(audio_path, model_path, top_k=5):
    print(f"ğŸ§ æ­£åœ¨å¤„ç†éŸ³é¢‘: {audio_path}")
    fixed_path = auto_convert_wav(audio_path)
    waveform = preprocess_audio(fixed_path)
    model = load_model(model_path)
    with torch.no_grad():
        logits = model(waveform)
        probs = torch.sigmoid(logits).squeeze()
        top_indices = torch.topk(probs, k=top_k).indices.tolist()
        return [{
    "tag": id_to_label[i][0],
    "zh": id_to_label[i][1],
    "probability": round(probs[i].item(), 3)
} for i in top_indices]

# å‘½ä»¤è¡Œæˆ–é»˜è®¤è¿è¡Œ
if __name__ == '__main__':
    try:
        result = predict_tags("audio.mp3", "scripts/baseline/models/best_model.pth")
        print("âœ… æ ‡ç­¾é¢„æµ‹ç»“æœï¼š")
        for tag in result:
            print(f" - {tag['tag']}ï¼ˆ{tag['zh']}ï¼‰: {tag['probability']}")
    except Exception as e:
        print("âŒ æ‰§è¡Œå¤±è´¥ï¼š", e)
        sys.exit(1)